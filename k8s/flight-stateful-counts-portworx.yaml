#
# You'll need to create a config map before creating this SparkApplication
#
# kubectl create configmap flight-stateful-conf --from-file=configMap/
#
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: spark-data
  annotations:
    volume.beta.kubernetes.io/storage-class: portworx-spark-sc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi
---
kind: StorageClass
apiVersion: storage.k8s.io/v1beta1
metadata:
  name: portworx-spark-sc
provisioner: kubernetes.io/portworx-volume
parameters:
  repl: "1"
  shared: "true"
---
apiVersion: "sparkoperator.k8s.io/v1beta1"
kind: SparkApplication
metadata:
  name: flight-stateful-counts
  namespace: default
spec:
  type: Scala
  mode: cluster
  image: "vplechnoy/apache-spark-testing:0.1"
  imagePullPolicy: Always
  mainClass: com.esri.realtime.stream.FlightCounter
  mainApplicationFile: "local:///opt/spark/apache-spark-testing-mvn.jar"
  arguments:
    - gateway-cp-kafka:9092
    - flight-stateful
  sparkVersion: "2.4.0"
  #sparkConfigMap: spark-conf
  sparkConf:
    "spark.executor.extraJavaOptions": "-Dlog4j.configuration=file:///opt/spark/log4j.properties"
    "spark.driver.extraJavaOptions": "-Dlog4j.configuration=file:///opt/spark/log4j.properties"
    "spark.driver.extraClassPath": "/opt/spark/dependency-jars/*"
    "spark.executor.extraClassPath": "/opt/spark/dependency-jars/*"
  restartPolicy:
    type: Always 
  volumes:
    - name: "spark-data"
      persistentVolumeClaim:
        claimName: spark-data
  driver:
    cores: 0.1
    coreLimit: "200m"
    memory: "512m"
    labels:
      version: 2.4.0
    serviceAccount: spark
    volumeMounts:
      - name: "spark-data"
        mountPath: "/data"
  executor:
    cores: 1
    instances: 3
    memory: "512m"
    labels:
      version: 2.4.0
    volumeMounts:
      - name: "spark-data"
        mountPath: "/data"
